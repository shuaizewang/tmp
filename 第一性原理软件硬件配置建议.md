# **常用第一性原理、量子化学、分子动力学软件的推荐硬件配置报告**

**1\. 引言**

在现代材料科学和化学领域，计算方法日益成为研究和开发的关键工具，能够对原子和分子水平上的复杂系统进行深入探索 1。这些模拟的效率和准确性在很大程度上取决于所使用的硬件基础设施，特别是中央处理器 (CPU)、内存和图形处理器 (GPU) 之间的协同作用 5。本报告旨在为使用 VASP、CP2K、Quantum ESPRESSO (QE)、Gaussian、Gromacs 和 Lammps 等主要软件的研究人员提供基于系统规模和理论方法水平的专家级硬件配置建议。

**2\. VASP 的硬件推荐**

* 2.1 CPU 核心数的影响：  
  VASP 的性能与 CPU 核心数的关系并非简单的线性扩展，而是受到内存带宽的显著影响 9。例如，在双路 64 核 EPYC Milan 系统上进行 DFT 计算的测试表明，由于内存带宽的限制，当 MPI 进程数达到 64 时，性能扩展趋于停滞 9。这表明，仅仅增加核心数量而不考虑内存子系统可能会导致收益递减和资源利用效率低下，尤其是在标准的 DFT 计算中。核心访问内存数据的速率成为限制因素，使得额外的核心处于空闲或利用不足的状态。这对于硬件采购策略具有重要意义，建议关注平衡的系统，而不是仅仅追求最大化核心数量。此外，对于某些计算，将每个节点的内核数从 8 个增加到 16 个可能只带来 25% 的性能提升 10。这暗示 VASP 的并行效率可能因计算类型和算法固有的并行性而显著不同。VASP 中某些计算任务可能具有显著的串行部分或通信开销，限制了增加更多核心带来的整体加速效果，因此对增加更多核心的成本效益分析至关重要。最佳核心数取决于具体的 CPU 架构和内存配置，强烈建议在目标硬件上进行基准测试 9。不同的 CPU 供应商和型号具有不同的内存通道数、带宽能力和核心间通信速度，所有这些都会直接影响 VASP 的性能，因此需要直接测试以确定最有效的配置。在现代高端系统上，使用最新的 CPU 和 InfiniBand 网络时，VASP 的扩展在达到大约 32 个处理器时可能还可以，但之后通常会显著下降 11。这进一步强调了非线性扩展的观察结果，并表明在更大规模的并行运行中，通信开销可能成为主要因素，尤其是在具有特定互连限制的系统上。随着处理器数量的增加，它们之间需要交换的数据量也会增加，而互连的速度对于维持性能扩展至关重要。像 InfiniBand 这样的快速互连可以显著降低通信开销，从而更好地扩展到更多节点。  
* 2.2 内存大小的影响：  
  内存大小在 VASP 计算中起着至关重要的作用，特别是对于高级方法 9。对于 GW 计算和其他复杂计算，建议使用 512GB 到 2TB 的内存 9。这揭示了基于所选理论方法的内存需求存在显著差异，高级电子结构方法需要明显更大的内存占用。GW 计算涉及四点相关函数的计算和存储，与标准 DFT 相比，内存需求要高得多。因此，在计划使用此类方法时，必须仔细考虑内存容量。VASP 的实际下限是每个核心 2GB 左右，而每个进程 256MB 可能不足以满足即使是小型系统的需求 12。这为内存分配建立了一个基准，强调了如果内存配置低于此阈值，可能会出现严重的性能瓶颈。内存不足会迫使操作系统在 RAM 和磁盘之间交换数据，这个过程比 RAM 访问慢几个数量级，导致计算时间大幅增加。虽然 512GB 的 RAM 对于大多数 DFT 计算可能足够，但 GW 计算可以从更大的内存容量中显著受益 9。这强调了根据特定计算任务调整内存资源的需求，避免为要求不高的方法过度配置，并避免为要求高的方法配置不足。理解不同 VASP 方法的内存扩展对于高效的资源分配和成本管理至关重要。评估光学性质时，即使对于只有几十个原子的系统，使用像 VASP 这样的平面波代码也可能需要超过 100GB 的内存，尤其是在使用 RPA 和 GW-BSE 等方法时 13。这说明即使在看似很小的系统上进行特定类型的性质计算，由于底层算法以及需要考虑大量电子态，也可能产生极端的内存需求。光学性质计算通常涉及对许多空电子带求和，而像 RPA 和 BSE 这样的方法会引入额外的复杂性，导致内存需求迅速增加。  
* 2.3 GPU 型号的作用：  
  GPU 在加速 VASP 计算方面发挥着越来越重要的作用，但加速的程度取决于具体的问题和算法 14。添加 2 个 K40 GPU 到双路 Sandy Bridge 计算节点时，观察到的加速范围为 1.4 倍到 8.0 倍，加速的程度取决于晶胞大小和所选算法 14。这表明 GPU 在 VASP 中具有显著的性能提升潜力，但也强调了加速效果取决于具体计算工作负载。GPU 加速的效率与底层算法如何并行化并映射到 GPU 的架构有关，这对于不同的计算类型和系统大小可能会有所不同。配备 NVIDIA A100 GPU 的 NERSC Perlmutter 超级计算机在 VASP 工作负载上表现良好，对于包含 256 个原子硅超晶胞的计算密集型 HSE 混合泛函，显示出良好的性能 15。这展示了现代高端 GPU 在加速 VASP 中像混合 DFT 这样的高级电子结构方法方面的有效性，尤其对于中等规模的系统。具有强大双精度浮点运算能力和高内存带宽的 GPU 非常适合混合 DFT 计算的需求。然而，对于较大的结构（大约 100 个原子），仅使用 48GB GPU 内存可能会出现内存问题 16。这强调了 GPU 内存容量对于在 GPU 上运行大规模 VASP 计算是一个关键的限制因素，因为整个波函数或其大部分需要驻留在 GPU 的高带宽内存中才能实现高效处理。超过 GPU 的内存容量可能会导致 CPU 和 GPU 之间性能严重下降的数据传输，甚至导致作业失败。提供的文本中缺乏关于不同 GPU 型号的具体用户体验 9。这表明虽然这些片段讨论了 GPU 加速的一般优势，但它们没有提供不同 GPU 硬件之间详细的比较。  
* 2.4 CPU 与 GPU 的性能比较：  
  分析 VASP 中 CPU 和 GPU 性能之间复杂的相互作用，需要考虑问题规模和理论方法。一位研究人员难以找到关于特定 CPU（Intel Xeon Gold 6530）和 GPU（NVIDIA L40S）产品用于 VASP DFT 计算的详细性能分析 16。这突显了在没有针对用户预期工作负载的特定基准测试以及缺乏关于不同 CPU-GPU 组合用于 VASP 的更全面的公开可用数据的情况下，很难进行直接的硬件比较。CPU 和 GPU 之间的最佳选择通常取决于多种因素，包括所使用的具体算法、系统的大小以及软件在每种硬件上的实现效率。与传统的基于 CPU 的计算相比，对于涉及精确交换计算的计算密集型案例，GPU 的加速效果显著，可达 5 到 20 倍 17。这强烈表明，GPU 加速为计算密集型且涉及高度并行化操作的方法（如混合密度泛函理论）提供了特别引人注目的优势。GPU 提供的巨大并行性非常适合精确交换计算中涉及的矩阵运算和傅里叶变换。对于标准工作负载，在单个节点上运行 GPU 上的 VASP 比仅使用 CPU 的作业提供了明显的性能改进，但对于较小的输入大小，跨多个 GPU 的可扩展性有限 18。这表明，当计算工作负载足够大以有效利用 GPU 的并行处理能力时，GPU 加速的优势最为明显，并且跨多个 GPU 的扩展可能会引入对于较小问题而言不总是有益的开销。管理多个 GPU 和在它们之间传输数据的开销可能会超过每个 GPU 工作负载过小时带来的性能提升。更计算密集型的作业往往更能从 GPU 加速中获益，而对于较小的作业，与单独在 CPU 上运行相比，CPU+GPU 上的性能开销可能会导致更差的性能 19。这再次强调了 GPU 加速的效率高度依赖于任务的计算强度，存在一个阈值，低于该阈值，开销会超过收益。将数据传输到 GPU 并启动内核的初始成本需要在足够大的计算量上进行摊销，才能获得净性能提升。在 GPU 节点上运行 VASP 通常会导致许多 CPU 核心处于空闲状态，并且对于非 HSE 工作负载，在这些空闲 CPU 资源上启用 OpenMP 线程可能会带来少量额外的加速 20。这表明混合 CPU-GPU 方法有可能提高整体资源利用率。虽然主要的计算负载在 GPU 上，但 CPU 可以处理其他任务或通过线程化为计算做出贡献。GPU 加速在处理较大的原子系统、需要多节点配置以及执行复杂的混合 DFT 计算时最为有益 21。这为 GPU 加速在 VASP 中的适用性提供了简洁的概述。  
* 2.5 进程绑定和内存带宽：  
  讨论进程亲和性和内存带宽在优化现代 HPC 系统上 VASP 性能方面的关键作用。对于具有非常高核心数的现代 CPU，无论供应商或架构如何，每个核心的内存带宽都是一个非常重要的瓶颈 9。这突显了一个基本的硬件限制，它会影响 VASP 在高核心数系统上的可扩展性，表明仅仅关注核心数量而不考虑内存子系统的容量来为这些核心提供数据可能会产生误导。VASP 是一种内存密集型应用程序，数据从内存传输到 CPU 核心的速率直接影响计算吞吐量。内存带宽不足会导致核心因数据匮乏而性能受限。当在多台机器上并行运行 VASP 时，机器之间的互连性（例如，InfiniBand、100GbE）非常重要，与仅通过千兆以太网连接的系统相比，情况大不相同 9。这强调了对于跨多个计算节点的大规模并行 VASP 模拟，网络互连的速度和延迟起着关键作用，因为节点之间的高效通信对于协调并行计算和交换数据至关重要。VASP 依赖 MPI 进行并行运行中的进程间通信，而这些通信的性能直接受网络带宽和延迟的影响。像 InfiniBand 这样的快速互连可以显著降低通信开销，从而更好地扩展到更多节点。Snippet 9 和 95 都强烈建议将进程绑定到特定的 CPU 核心，以防止进程在模拟过程中在核心之间迁移。这种优化技术对于保持数据局部性和最大限度地减少因上下文切换和缓存未命中而导致的性能下降至关重要，如果允许进程在系统上的不同核心之间自由移动，则可能会发生这种情况。当进程移动到不同的核心时，它正在积极使用的数据可能不再存在于新核心的缓存中，导致延迟增加，因为需要再次从主内存中获取数据。绑定进程可确保它们保留在同一核心上，从而最大限度地提高缓存命中率并提高整体性能。Snippet 10 推荐使用英特尔的服务器处理器作为 VASP 的最佳选择，因为 VASP 非常依赖英特尔的 Fortran 编译器，并且英特尔处理器在性能和能源效率方面都处于领先地位。这表明编译器优化和特定的 CPU 架构特性可能会影响 VASP 的性能，可能使某些 CPU 供应商或型号更具优势。VASP 的代码库主要用 Fortran 编写，可能受益于英特尔 Fortran 编译器提供的优化，该编译器通常针对英特尔的 CPU 架构进行了优化。Snippet 11 指出，虽然当前英特尔至强处理器的内存带宽与过去相比有所提高，但它仍然可能是一个潜在的瓶颈，尤其对于内存密集型的 VASP 计算而言。即使 CPU 和内存技术取得了进步，内存带宽仍然是配置 VASP 硬件时需要考虑的关键因素，特别是对于大规模模拟或采用具有高内存访问模式的方法的模拟。

**3\. CP2K 的硬件推荐**

* 3.1 CPU 架构：  
  讨论 CP2K 在不同 CPU 架构上的不同性能，强调需要针对特定架构进行考虑 22。Snippet 22 表明，对于特定的材料科学应用，尽管进行了并行化努力，CP2K 在 Xeon Phi 架构上的性能比 Sandy Bridge 16 核 CPU 节点慢 4 倍。这表明，特定 CPU 架构是否适合 CP2K 不仅取决于核心数或理论峰值性能，还取决于软件的算法与架构特性（如向量处理能力和内存层次结构）的契合程度。Xeon Phi 架构具有多核设计和宽向量单元，可能没有被所测试的特定 CP2K 例程充分利用，这表明可能需要代码优化或不同的并行化策略才能充分发挥其潜力。Snippets 23 和 24 报告了使用 CP2K 在英特尔至强 CPU 上进行的从头算分子动力学模拟的扩展研究，结果显示，通过适当的进程放置和亲和性设置，扩展性良好（达到理想扩展性的 70% 或更高）达 10 个计算节点。这表明，英特尔至强处理器在正确配置的情况下，可以为 CP2K 提供强大的并行性能，尤其对于分子动力学模拟而言，这是该软件的一个重要应用领域。有效利用 MPI 进程和 OpenMP 线程，并仔细考虑 NUMA 域和处理器绑定，对于在多路至强系统上实现良好的扩展性至关重要。Snippet 25 提到，采用高带宽内存 (HBM2e) 的英特尔至强 Max 处理器可以为受内存带宽限制的 CP2K 工作负载（如水簇上的 RPA 和 MP2 计算）提供高达 3 倍的代际加速。这突显了高带宽内存技术在解决像 CP2K 这样内存密集型计算化学应用中的性能瓶颈方面的日益重要性，尤其对于高级电子结构方法而言。对于内存数据访问速率是主要限制因素的计算，配备 HBM 的 CPU 可以通过减少内存访问延迟和提高数据传输速率来提供显著的性能优势。Snippet 26 展示了基准测试结果，其中 64 核第四代和第五代 AMD EPYC 处理器在运行 CP2K H2O-dft-ls-NREP6 基准测试时，与双路配置的顶级第五代英特尔至强铂金 8592+ 处理器相比，提供了高达 1.64 倍的性能提升。这表明，AMD 最新一代的 EPYC 处理器也可以为 CP2K 提供有竞争力的性能，在某些情况下甚至更胜一筹，尤其对于密度泛函理论计算而言，这突显了 AMD 硬件在用于计算化学的 HPC 领域中日益增长的可行性。现代 EPYC 处理器的架构特性，如高核心数、通过 DDR5 支持提高的内存带宽以及高效的核心间通信，都有助于其在像 CP2K 这样要求苛刻的应用中表现出色。  
* 3.2 内存带宽：  
  分析内存带宽在 CP2K 性能中的关键作用，尤其对于基于网格的操作和大规模模拟 22。Snippet 22 讨论了在 CP2K 中降低 MGRID%CUTOFF 值以减少与电子密度平面波展开所使用的 3D 网格相关的内存占用的策略，指出这需要在精度和降低内存及运行时间之间进行权衡。这揭示了内存限制可能会迫使 CP2K 计算在精度上做出妥协，强调了拥有足够的内存带宽来支持高保真模拟的重要性。更大的 MGRID 截止值会导致电子密度的更精确表示，但也导致更大的网格尺寸和更高的内存需求。内存带宽不足会加剧这些大型网格的性能影响。Snippet 25 明确指出 CP2K 中“几个”工作负载从高内存带宽中获益显著，表明这是影响软件大部分功能性能的关键因素。这强化了内存带宽不仅是特定方法的关注点，而且对 CP2K 在各种计算类型中的整体性能都有更广泛影响的理解。CP2K 中的许多核心算法，包括涉及实空间网格、快速傅里叶变换和稀疏矩阵运算的算法，本质上都是内存密集型的，需要快速高效的数据访问。Snippet 27 通过指出即使是 HPC 中最先进的 GPU，对于 HPC 模拟和建模（包括 CP2K）等工作负载而言，也常常“迫切需要内存带宽和内存容量”，从而扩展了视角。这第三层洞察将内存带宽置于高性能计算中的一个基本挑战，不仅影响 CPU，还影响日益强大的 GPU 加速器，这表明内存技术的进步对于科学应用的未来性能提升至关重要。随着计算能力的持续提高，以足够快的速度将数据移入和移出处理单元的能力成为一个关键瓶颈，限制了可以实现的整体性能。  
* 3.3 GPU 加速：  
  详细介绍 CP2K 中 GPU 加速的支持和性能影响，同时指出显著加速的潜力以及对仔细配置和问题规模考虑的需求 23。Snippets 23 和 24 报告称，与最快的仅 CPU 运行相比，在具有两个 NVIDIA V100 GPU 的单个节点上使用 CP2K 进行从头算分子动力学模拟，获得了超过 3.7 倍的显著加速，并且比五个仅 CPU 节点上的最快时间快 13%。这第二层洞察表明，GPU 加速具有显著潜力，可以大幅缩短 CP2K 进行复杂分子动力学模拟所需的时间，为解决更大规模的系统和更长时间尺度的问题提供了途径。MD 模拟中力计算的高度并行性非常适合现代 GPU 的架构，允许同时计算大量相互作用，从而带来显著的性能提升。Snippet 28 描述了一位用户在具有 8 个 GPU 的单节点基准测试中，在使用 NVIDIA V100 SXM2 GPU 时遇到非常低的性能，甚至比仅 CPU 基准测试还要慢，这表明在 CP2K 中实现最佳 GPU 性能可能具有挑战性，并且可能取决于问题规模和特定的代码编译选项等因素。这突显了 CP2K 中的 GPU 加速并非总是简单的“即插即用”解决方案，需要仔细注意配置、计算问题的规模以及可能被加速的特定代码部分。CPU 和 GPU 之间的数据传输开销以及 GPU 加速内核对于给定工作负载的效率，都会显著影响整体性能。对于较小的问题，这种开销可能会超过使用 GPU 的计算优势。Snippet 25 提到，CP2K 中通过 DBM（分布式块矩阵）库实现的线性标度 DFT (LS-DFT) 经常显示出良好的 GPU 加速效果，表明 CP2K 中特定的算法实现已针对 GPU 架构进行了优化。这表明 CP2K 的开发人员已做出有针对性的努力，以利用 GPU 加速特定的计算方法，如 LS-DFT，这对于实现更大规模系统的模拟非常重要。DBCSR 库（已进行 GPU 加速）在 CP2K 中 LS-DFT 的性能中起着关键作用，允许在 GPU 上高效地并行化矩阵运算。Snippet 29 确认 CP2K 支持各种 NVIDIA GPU，包括 Pascal、Volta、Ampere 和 Hopper 架构，以及 ARM64 架构的 GPU，为用户选择 GPU 硬件提供了灵活性。Snippet 30 提供了一个实用技巧，建议为了在使用 GPU 和 CP2K 时获得最佳性能，通常最好每个 GPU 使用一个 MPI 进程，以避免过度线程化和潜在的资源争用。这为配置 CP2K 中的并行执行以最大化 GPU 利用率的效率提供了一个具体的指导方针。  
* 3.4 DFT 和 MP2 方法：  
  讨论在 CP2K 中运行 DFT 和 MP2 计算的硬件考虑因素，强调这些方法不同的计算需求 25。Snippet 31 展示了在 ARCHER 超级计算机上使用线性标度 DFT (H2O-DFT-LS) 进行单点能量计算的基准测试结果，最快时间使用了大量核心 (49152)，表明 CP2K 中某些类型的 DFT 计算可以有效地利用基于 CPU 的 HPC 系统上的大规模并行性。这第二层洞察表明，CP2K 中线性标度 DFT 方法在具有大量核心的 CPU 架构上具有良好的可扩展性，突显了它们对于大型系统的适用性，对于这些系统，使用传统的立方标度 DFT 计算成本将非常高昂。线性标度 DFT 算法通过关注局部相互作用并采用诸如稀疏矩阵表示的技术，可以将计算工作负载分布到非常大量的处理器上，且通信开销相对较低。Snippet 31 还包括 H2O-64-ri-mp2 的基准测试结果，这是一个解析恒等式 MP2 单点能量计算，并指出对于相同的系统，其计算成本比标准 DFT 高约 100 倍，并展示了在各种利用数万个核心的超级计算架构上的性能。这与 DFT 形成鲜明对比，强调了 CP2K 中 MP2 方法明显更高的计算需求，即使对于相对较小的系统，也需要大量资源和更长的运行时间，使其成为通过 GPU 或高性能 CPU 加速的主要候选方法。传统 MP2 方法的计算成本随基函数数量的五次方 (O(N^5)) 增长，这使得它们对于较大的系统而言计算量非常大，因此需要高性能计算资源才能实现可行的运行时间。Snippet 25 提到，在采用高带宽内存的英特尔至强 Max 处理器上，CP2K 在 32-H2O/RPA/MP2 计算中显示出加速效果，表明这些高级方法可以从这种架构提供的更高内存带宽中获益。Snippet 32 提供了在各种 HPC 系统上对 64 个水分子系统进行的 RI-MP2 和 RI-dRPA（解析恒等式密度拟合 RPA）计算的详细基准测试结果，进一步说明了高计算成本以及性能对底层硬件架构和所用核心数的依赖性。Snippet 33 明确指出 MP2 相关在 CP2K 中的成本为 O(N^5)，而 RPA 的成本为 O(N^4)，突显了当从标准 DFT 方法转向包含更精确的电子相关处理时，计算要求的急剧增加。

**4\. Quantum ESPRESSO (QE) 的硬件推荐**

* 4.1 CPU 类型：  
  分析 QE 的 CPU 类型建议，从主流高端到专用 HPC 处理器 34。Snippet 34 建议英特尔酷睿 i7 或锐龙 7 处理器作为计算（包括 QE）入门级 PC 的良好起点。这表明，对于入门级或较小规模的 QE 计算，现成的高性能桌面 CPU 提供了性能和成本的平衡。这些 CPU 提供了足够的核心数和合适的时钟速度，可以处理许多常见的 QE 任务，而无需专门的 HPC 硬件。Snippet 35 讨论了考虑使用 AMD 锐龙 9 7950X3D 或英特尔酷睿 i9-13900K 作为仅 CPU 的 QE 工作站，并指出基准测试表明，对于某些工作负载，这些高核心数 CPU 可能与更昂贵的至强或线程撕裂者处理器媲美甚至更好。这突显了顶级消费级 CPU 对于要求苛刻的 QE 任务的可行性，根据具体应用以及所需核心数和时钟速度之间的平衡，它们可能提供比传统工作站或服务器处理器更具成本效益的解决方案。对于可以有效利用大量并行核心的 QE 计算，这些具有竞争力的核心数和高时钟频率的高端桌面 CPU 可以提供出色的性能。Snippet 36 的性能基准测试显示，在运行 Quantum ESPRESSO qe-7.0\_Ausurf 工作负载时，由 192 核第五代 AMD EPYC 9965 处理器驱动的双路系统与由 64 核第五代英特尔至强 8592+ 处理器驱动的双路系统相比，性能提升高达约 2.40 倍；而在运行 qe-7.0\_Ta205 工作负载时，由 96 核第四代 AMD EPYC 9654 处理器驱动的双路系统与相同的英特尔系统相比，性能提升约 1.16 倍。这提供了强有力的证据，表明 AMD EPYC 处理器，尤其是最新一代具有高核心数和内存带宽的处理器，可以为各种类型的 QE 计算提供显著的性能优势，使其成为运行 QE 的 HPC 环境中极具吸引力的选择。  
* 4.2 内存容量：  
  讨论内存容量对于 QE 的重要性，尤其对于高级计算和大型系统 13。Snippets 13 和 13 都表明，与 VASP 类似，即使对于只有几十个原子的结构，使用 Quantum ESPRESSO 评估光学性质也可能需要超过 100GB 的 RAM，尤其是在使用 RPA（随机相位近似）和 GW-BSE（贝特-萨尔皮特方程）等方法时。这第二层洞察强调了 QE 中特定的后 DFT 计算（关注激发态性质）非常耗费内存，需要仔细规划，并可能需要访问高内存计算节点。这些高级方法涉及更复杂的理论框架，需要存储和操作更大的数据集，例如介电函数和双粒子格林函数，导致内存消耗迅速增加。Snippet 37 明确提到，QE 的 GPU 加速版本几乎完全在 GPU 内存中运行，GPU 内存通常在 16GB 到 32GB 之间，因此直接限制了单个 GPU 上可以处理的计算问题的大小，而与主节点的内存容量无关。这突出了 QE 中基于 CPU 和基于 GPU 的执行之间的一个关键区别，对于 GPU 加速，GPU 卡上的可用内存成为大型模拟的主要瓶颈。GPU 计算的效率依赖于使数据尽可能靠近处理单元。对于 QE 的 GPU 实现，这意味着波函数和其他基本数据结构需要适合 GPU 的高带宽内存。Snippet 38 在提及 QuantumATK（它使用 QE 作为其引擎）时建议，对于并行计算，推荐每个核心 8 到 16 GB 的 RAM。这为在多核处理器上并行运行 QE 时估计所需的总系统内存提供了一个实用的经验法则，确保每个核心都有足够的内存来高效运行，而不会遇到内存不足错误或过度交换。  
* 4.3 GPU 加速：  
  详细介绍 QE 中 GPU 加速的当前状态，重点介绍其功能和硬件要求 37。Snippet 39 概述了 Quantum ESPRESSO 套件已使用混合 CUDA Fortran/OpenACC 方案针对 NVIDIA GPU 进行了加速，并且正在大力开发基于 OpenMP 卸载的版本，以增强在不同供应商（AMD、Intel）硬件上的可移植性。这表明 QE 在更广泛的 GPU 支持方面采取了战略方向，利用 NVIDIA 特有的技术和更具可移植性的标准（如 OpenMP）来实现在更广泛的硬件上进行加速。Snippet 37 推荐使用特定的 NVIDIA TESLA GPU，如 Kepler (K20, K40, K80)、Pascal (P100) 和 Volta (V100)，并强烈推荐 P100 和 V100，因为它们具有更大的板载内存容量和卓越的双精度浮点性能，这对于准确的 DFT 计算至关重要。这突显了对于要求苛刻的 QE 计算，首选具有充足内存和强大 FP64 功能的专业级 NVIDIA GPU，以实现最佳性能和精度。Snippet 40 指出，最新版本的 Quantum ESPRESSO (qe-7.2) 已将 GPU 执行扩展到包括线性响应代码（如 PHonon、turboEELS 和 HP）以及分子动力学代码 CP，这标志着 GPU 加速在 QE 套件中各种功能中的应用日益广泛。Snippet 41 建议，由于消费级 NVIDIA GPU 在硬件上对双精度算术的支持可能有限，因此它们可能不太适合进行准确的 DFT 模拟，而双精度算术对于这些计算通常是必不可少的。这意味着对于使用 QE 进行的研究级工作，尤其是那些需要高精度的研究，与依赖消费级显卡相比，投资具有强大双精度能力的专业 GPU 通常是明智之举。Snippet 42 提到，虽然 Quantum ESPRESSO 以其在各种硬件（包括 GPU）上的性能而闻名，但论文中介绍的基准测试侧重于相对较小的机器，并未旨在展示在非常大型系统上的性能，这表明 QE 在大规模并行 GPU 架构上的扩展行为可能需要进一步研究。Snippet 43 讨论了从头开始基于 CUDA Fortran 开发的与 QE 6 版本兼容的全新版本，重点是在大规模和密集型 GPU 系统配置上提供高性能，所有重要的计算都在 GPU 上进行。  
* 4.4 LDA、GGA 和混合泛函：  
  讨论在 QE 中使用不同密度泛函的硬件影响，注意精度和计算成本之间的权衡 44。Snippet 44 解释说，虽然 LDA 和 GGA 泛函在计算上是高效的，但它们通常在精度方面存在局限性，例如过度结合 (LDA) 或结合不足 (GGA)，以及对强相关材料和范德华相互作用的处理不正确，而 meta-GGA 和混合泛函提供了改进，但可能在数值上不稳定，并且需要更密集的 FFT 网格，从而增加了计算需求。这第二层洞察突显了 DFT 中计算效率和所选交换关联泛函精度之间的基本权衡，更高的精度通常需要更苛刻的硬件资源。超越局部密度和梯度近似的泛函，例如 meta-GGA（取决于动能密度）和混合泛函（包含一部分精确交换），会给计算带来额外的复杂性，导致计算时间和内存使用量增加。Snippet 45 描述了在 QE 中使用 HSE（Heyd-Scuseria-Ernzerhof）混合泛函进行能带结构计算的工作流程，该工作流程涉及具有 Fock 算符网格特定参数 (nqx) 的收敛 SCF 计算，然后展开简化的 k 点网格和 Wannier 化，表明混合泛函计算需要仔细的参数设置，并且可能比标准 LDA 或 GGA 运行更复杂。Snippet 46 比较了 LDA、PBE（GGA 泛函）和 SCAN（meta-GGA 泛函）在预测半导体和绝缘体带隙方面的性能，结果表明，虽然 LDA 和 PBE 显示出较大的平均误差和平均绝对误差，但 SCAN 提供了显著提高的精度，但通常伴随着更高的计算成本。Snippet 47 提供了关于在 QE 输入文件中使用 input\_dft 关键字指定交换关联泛函的指南，允许用户在不同的 LDA 和 GGA 泛函之间进行选择，并提到了使用 Libxc 访问更广泛泛函的可能性，同时指出 input\_dft 的语法可能因 QE 版本而异。Snippet 48 讨论了标准 LDA/GGA 泛函在带隙低估方面的常见问题，并提到通常采用计算成本更高的 GW 校正或混合交换关联泛函来解决此限制，进一步强调了更高精度带来的硬件需求增加。  
* 4.5 并行化策略：  
  简要提及 QE 中的并行化 37。Snippet 37 和 49 讨论了使用 QE 中 \-npool 选项对 k 点进行 MPI 并行化。这突出了消息传递接口 (MPI) 在使 QE 能够在并行计算架构上高效运行、将工作负载分布到多个处理器或节点中的基本作用。通过将 k 点集（代表布里渊区的采样）分配给不同的 MPI 进程，QE 可以同时执行倒易空间不同部分的计算，从而显著缩短总计算时间。Snippet 49 进一步阐述了 QE 中实现的多个并行化级别，包括 world、images、pools（超过 k 点）、bands、PW（平面波基集分布）和 tasks（用于高效并行化 3D FFT），并指出最佳处理器数量的选择取决于与系统大小、算法和硬件架构相关的几个因素。这突显了 QE 复杂的并行计算能力，为用户提供了一系列策略，以根据其模拟的具体特征和可用的计算资源优化性能。

**5\. Gaussian 的硬件推荐**

* 5.1 CPU 主频：  
  讨论 CPU 主频对于 Gaussian 性能的重要性 50。Snippet 50 明确指出，处理器时钟频率越快，Gaussian 计算往往越快，CPU 主频与计算时间之间呈现或多或少的线性关系。这第二层洞察强调，对于使用 Gaussian 进行的许多类型的量子化学计算，尤其是那些不能完美地随着核心数量扩展的计算，最大化单个 CPU 核心的时钟速度可能是降低整体运行时间的至关重要因素。更高的时钟速度允许 CPU 每秒执行更多指令，直接转化为更快地完成 Gaussian 代码的串行或并行化程度较低的部分。这对于较小的分子或受单核性能限制的计算尤其重要。  
* 5.2 内存大小：  
  分析 Gaussian 的内存需求对系统大小、基组和所用处理器数量的依赖性 51。Snippet 51 建议，对于涉及 50 个以上原子和/或 500 个以上基函数的 Gaussian 计算，每个处理器至少需要 4GB 的 RAM，这表明并行作业的总内存分配应随着所用 CPU 数量的增加而线性扩展。这第二层洞察强调了在并行 Gaussian 计算中为每个核心提供足够内存的重要性，以避免由于内存争用或各个线程的工作空间不足而导致性能下降。随着原子和基函数数量的增加，Gaussian 需要管理的数据结构的大小也会增加，当这些计算分布在多个核心上时，每个核心都需要足够的内存来高效地处理其分配的工作负载。Snippet 52 指出，64 位版本的 Gaussian 16W 对其可以访问的内存量没有硬性限制，使其能够利用现代计算系统中可用的大量 RAM 容量来处理非常大的计算。Snippet 53 提到，虽然 Gaussian 默认分配 800 MB 内存，但对于较大的计算可能不够，用户应使用 %Mem Link 0 命令调整内存分配，同时警告不要请求超过系统物理内存量的内存，因为这会导致由于过度交换而导致严重的性能问题。Snippet 54 提供了一个基于计算类型、基函数数量 (N) 和核心数 (n) 估算 Gaussian 计算所需内存的公式，表明内存需求随着系统大小（反映在 N 中）和并行级别 (n) 的增加而增加，尤其对于 64 位系统。  
* 5.3 磁盘 I/O 速度：  
  讨论快速磁盘 I/O（尤其是 NVMe SSD）在增强磁盘 I/O 密集型 Gaussian 作业（如采用高度相关方法（如 MP2 和 CCSD(T)）以及振动频率计算的作业）性能方面的显著作用 50。Snippet 55 明确指出，对于 Gaussian 中磁盘 I/O 密集型的作业，利用 Puhti 或 Mahti 等 HPC 集群上可用的快速 NVMe 本地磁盘可以显著提高整体性能，方法是减少读取和写入大型临时文件所花费的时间。这第二层洞察强调，Gaussian 中某些高级量子化学方法的性能不仅受 CPU 速度或内存容量的限制，还受系统访问和操作存储设备上数据的速度的限制。像 MP2 和 CCSD(T) 这样的方法通常会生成大量的中间数据，这些数据在计算过程中会被写入磁盘。更快的磁盘 I/O（如 NVMe SSD 提供的）可以显著减少与这些操作相关的延迟，从而缩短整体运行时间。Snippet 50 还提到，Gaussian 生成的读写文件 (RWF)，尤其是对于 MP2 频率计算以及甚至对于较大系统上的 DFT 频率计算，可能会占用大量磁盘空间，进一步强调了对充足且快速的存储资源的需求。  
* 5.4 DFT、HF、MP2、CCSD 方法：  
  分析 Gaussian 中不同量子化学方法特定的硬件考虑因素，注意它们对 CPU、内存和磁盘资源的不同需求 51。Snippet 56 列出了 Gaussian 中可用的各种电子结构方法，包括密度泛函理论 (DFT)、Hartree-Fock (HF)、Møller-Plesset 微扰理论 (MP2) 和耦合簇方法（如 CCSD），表明该软件广泛适用于各种级别的量子化学研究。Snippet 57 提供了关于 GPU 加速在 Gaussian 中有效性的具体指导，指出 GPU 在处理较大分子时进行 DFT 能量、梯度和频率（包括基态和激发态）计算时有效，但对于小型作业或后 SCF 方法（如 MP2 或 CCSD）则无效。这第二层洞察揭示了 GPU 加速在 Gaussian 中的优势高度依赖于所使用的方法，主要针对 DFT 计算中计算密集型的部分，而其他高级方法可能仍然主要依赖 CPU 资源。用于后 SCF 方法（如 MP2 和 CCSD）的算法可能在 Gaussian 的当前实现中不易在 GPU 架构上并行化，或者将数据传输到 GPU 和从 GPU 传输数据的开销可能会超过这些方法的计算优势。Snippet 58 表明，对于使用 Linda 环境跨多个节点进行的并行执行，Gaussian 中的 HF、DFT、TDDFT 和 MP2（能量和梯度）计算可以有效地分布，而 MP2 频率和 CCSD 计算仅部分 Linda 并行，这表明它们的扩展性可能在超过一定数量的节点后受到限制。Snippet 51 提到，只要有足够的内存并且线程绑定到特定的核心，对于大型分子，Gaussian 的并行效率通常在高达 64 个或更多核心的情况下都很好，突显了该软件能够利用高性能计算资源进行要求苛刻的计算。Snippet 59 指出，对于像 MP2 这样的后 HF 方法，性能很大程度上取决于系统的各个方面，包括利用“incore”选项的硬件资源可用性，如果存在足够的 RAM，该选项可以将积分保留在内存中并显著提高性能。  
* 5.5 并行处理：  
  简要讨论 Gaussian 的并行执行能力，这对于利用多核处理器和 HPC 集群的强大功能至关重要 53。Snippet 55 和 53 解释说，Gaussian 支持单节点上的共享内存并行 (SMP) 执行（使用 %NProcShared 或 %CPU Link 0 命令指定核心数）以及使用 Linda 环境跨集群中多个节点的分布式内存并行执行。Snippet 60 建议在 Alpine 等 HPC 集群上运行多节点并行作业时，每个节点使用一个 Linda 工作进程，每个节点利用多个 (最多 64 个) SMP 核心，以最大化资源利用率和性能。

**6\. Gromacs 的硬件推荐**

* 6.1 CPU 核心数：  
  分析 Gromacs 随 CPU 核心数增加而呈现的通常为非线性扩展的特性，强调高时钟速度的重要性以及在与 GPU 配对时超出一定核心数后的收益递减 61。Snippet 61 建议，对于分子动力学工作负载（包括使用 Gromacs 运行的工作负载），选择 CPU 的关键在于优先考虑处理器时钟速度而不是核心总数，推荐使用 32 到 64 个核心的 CPU，并建议采用单 CPU 部署以避免双 CPU 服务器中的通信延迟问题。这第二层洞察表明，对于许多 MD 模拟而言，单个核心的响应速度和速度比仅仅拥有大量核心更为重要，这表明具有高时钟频率的单路 CPU 配置可能更适合在 Gromacs 中实现最佳性能。MD 模拟中涉及的计算，特别是力计算和邻居列表更新，通常具有限制其在非常大量的核心上有效并行化的依赖性。更快的单个核心可以更快地处理这些任务，从而提高整体性能。Snippet 62 报告称，Gromacs 在超出一定限度后，其性能不会随着额外 CPU 的增加而显著提升，并且对于给定的 CPU，更高的核心数也不一定会带来更好的性能，尤其是在使用 GPU 加速的情况下，CPU 的作用可能不那么计算密集。Snippet 63 提到，最近发布的 Gromacs 版本在与高性能 GPU 配对时，只需几个 CPU 核心即可高效运行，这表明计算密集型的 MD 模拟部分越来越依赖 GPU 加速。Snippet 64 描述了一位用户在 HPC 系统上使用两个节点（64 个核心）运行 Gromacs 模拟时，性能比使用单个节点（32 个核心）时更慢的经历，突显了多节点扩展的复杂性以及如果管理不当，通信开销可能会抵消增加核心数带来的好处。  
* 6.2 内存大小：  
  讨论 Gromacs 模拟通常适中的 RAM 需求，同时指出更大的内存容量对于模拟前后的分析可能是有益的 65。Snippet 65 建议，虽然 16 GB 的 RAM 通常足以满足分子动力学模拟本身的需求，但拥有 32-64 GB 的 RAM 对于未来的需求（如同时加载和运行多个分析工具或处理更大的系统）将是有利的。这第二层洞察区分了核心模拟所需的内存与可以通过促进更复杂的分析以及同时处理多个数据集或应用程序的能力来增强整体研究工作流程的额外内存。Snippet 66 指出，对于进行 MD 模拟，32GB 的内存通常足够，即使使用超过 16 个核心也是如此，并且模拟甚至可以在配置较低的系统上运行，尽管性能可能会受到影响。Snippet 67 引用了用于 Gromacs 基准测试的硬件配置，其中包括每个节点 8 GB 到 64 GB 的 RAM 容量，表明具体的内存需求可能因模拟系统的大小和复杂性而异。  
* 6.3 GPU 型号：  
  分析高性能 NVIDIA GPU 在加速 Gromacs 模拟中的重要作用，并注意具体的推荐型号以及 CUDA 支持的重要性 61。Snippet 61 推荐一系列高端 GeForce RTX GPU（如 RTX 4090 和 4080）以及中端 NVIDIA 专业 RTX GPU（如 RTX 6000 Ada、RTX 5000 Ada 和 RTX 4500 Ada）作为分子动力学模拟（包括使用 Gromacs 运行的模拟）最受欢迎和最有效的选择。这第二层洞察突显了 NVIDIA 在提供非常适合并高度优化用于 MD 模拟的 GPU 方面的领先地位，尤其通过其 CUDA 技术，使其成为寻求在 Gromacs 中获得显著加速的研究人员的首选供应商。NVIDIA 的 GPU 提供了高核心数、快速时钟速度和专门功能的组合，这些功能与 MD 力计算中以及 Gromacs 中使用的其他关键算法的计算需求非常吻合。Snippet 62 报告称，虽然性能最强大的 GPU（RTX 6000 Ada）在 Gromacs 基准测试中并不总是提供最佳性能，但中端专业 RTX A5500 通常以更低的成本达到与其相近的性能，而 RTX A4500 也提供了极具吸引力的性价比。Snippet 67 展示了基准测试结果，表明与仅使用 CPU 的配置相比，使用各种 NVIDIA GPU（Tesla K20X、K40、GTX 680、770、780、Titan）进行 Gromacs 模拟时，性能得到了显著提升（例如，数倍的加速）。Snippet 68 提供了一个详细的基准测试，比较了不同代 NVIDIA GPU（GTX 1080Ti、RTX 2080Ti、V100、RTX 3080、A40、A100）在不同大小的 Gromacs 模拟中的性能，结果表明，更新的架构（如 Ampere，即 A40 和 A100）由于 CUDA 核心数和内存带宽的增加，为大型系统提供了显著的性能优势。Snippet 69 建议，尽管 NVIDIA 的 Quadro 系列 GPU 主要用于可视化任务，但它们也可以有效地用于 Gromacs 中的计算，为用户提供了另一个选择，具体取决于他们的特定需求和硬件可用性。  
* 6.4 大型分子模拟：  
  讨论在大型分子系统上运行 Gromacs 的硬件要求，强调对于数百万个原子的系统，需要强大的 GPU 和可能的多节点集群 66。Snippet 70 提到，Gromacs 主要用于生物化学分子，如蛋白质、脂类和核酸，这些分子通常构成包含许多原子的大型复杂系统。Snippet 66 指出，对于涉及数百万个原子的 MD 模拟，需要使用具有多个节点的计算机集群进行并行计算，以实现可行的模拟时间。Snippet 71 提供了一个用于 Gromacs 性能测试的基准系统列表，其中包括像 benchPEP 和 benchPEP-h 这样的示例，这些示例涉及包含约 1200 万个原子的大型系统，突显了该软件在拥有足够的计算资源时处理如此大规模模拟的能力。Snippet 72 讨论了在包含 50 万个原子的大型系统上运行 Gromacs 时遇到的扩展限制，即使使用 GPU 加速也是如此，这表明对于非常大的系统，CPU-GPU 互连带宽可能成为一个重要的瓶颈，限制了通过添加更多 GPU 或 CPU 核心获得的进一步性能提升。  
* 6.5 CPU-GPU 比率和多 GPU：  
  分析 Gromacs 的 CPU 和 GPU 资源之间的最佳平衡以及使用多个 GPU 增强性能的有效性 61。Snippet 63 建议，多年来，Gromacs 的经验法则是每个 GPU 需要几个更快的桌面/工作站 CPU 核心（或 1.5-2 倍的服务器核心）才能接近峰值性能，尽管最近的版本在与高性能 GPU 配对时，每个 GPU 可能只需更少的核心即可良好运行，具体取决于模拟类型和工作负载。Snippet 61 指出，虽然 GPU 核心数和时钟速度是 Gromacs 加速的主要优先事项，但该软件确实会利用 CPU，这表明需要一种平衡的方法，即充分配置 CPU 和 GPU 资源。它还提到，Gromacs 工作负载对于单个模拟而言，通常不会随着更多 GPU 的增加而更好地扩展，并且多 GPU 系统通常用于同时运行多个独立的作业以提高整体吞吐量。Snippets 73 和 74 提供了如何使用 \-gpu\_id 选项在单个节点上配置 Gromacs 以使用多个 GPU 的示例，表明内置了对利用多个加速器的支持。Snippet 68 观察到，对于 Gromacs 中的非常大的系统，在某些情况下，多 GPU 设置可能是有益的，但这需要根据具体情况进行评估，因为性能会受到模拟类型和大小以及 CPU 和 GPU 之间互连等因素的影响。

**7\. Lammps 的硬件推荐**

* 7.1 CPU 核心数：  
  分析 Lammps 性能对 CPU 核心数的依赖性，注意需要进行基准测试以找到给定系统和工作负载的最佳数量 61。Snippet 61 建议，对于分子动力学模拟（包括使用 Lammps 运行的模拟），与 Gromacs 的建议类似，优先考虑处理器时钟速度而不是核心数通常是有益的。Snippet 75 强调，Lammps 的最佳处理器数量很大程度上取决于各种因素，如原子数、所选算法、实现细节、编译优化和硬件，强烈建议用户在其特定的测试系统上使用不同的核心数进行基准测试，以确定最有效的配置。Snippet 76 展示了 Lammps 的 CPU 扩展基准测试结果，结果表明，虽然性能通常随着核心数的增加而提高，但加速并非总是线性的，并且并行效率往往随着更多核心的使用而降低，尤其是在更高的核心数下（例如，从 16 个核心到 32 个核心的性能提升小于从 1 个核心到 4 个核心的性能提升）。  
* 7.2 内存大小：  
  分析 Lammps 的内存考虑因素，注意精度要求可能产生的影响 61。Snippet 61 在分子动力学模拟（包括与 Lammps 相关的模拟）的背景下建议，顶级消费级 CPU 和中端工作站/数据中心 CPU 之间的主要区别在于 RAM 容量和 PCIe 通道数，这意味着对于非常大的 Lammps 模拟或需要大量数据处理的模拟，可能需要具有更高内存限制的工作站或服务器级 CPU 平台。Snippet 77 提到，对于使用 Lammps 模拟的非常大的系统，强烈建议使用双精度进行计算以保持精度，这可能会增加总体内存占用，而单精度运行则不会。Snippet 76 提供了 Lammps 的 GPU 扩展基准测试，但没有明确详细说明这些运行的 CPU 内存要求。  
* 7.3 GPU 加速：  
  详细介绍 Lammps 中的 GPU 加速功能，重点介绍其灵活性以及对不同 GPU 供应商和编程模型的支持 61。Snippet 61 推荐使用高端 GeForce RTX 和中端 NVIDIA 专业 RTX GPU 进行分子动力学模拟，这通常也适用于 Lammps，表明强大的 NVIDIA GPU 是加速 Lammps 模拟的不错选择。Snippet 77 报告了一位用户发现，对于使用 Lammps 模拟的简单 Lennard-Jones 熔化和聚合物系统，CPU 计算机比他们拥有的 GPU 更快，这强调了 GPU 加速的有效性很大程度上取决于系统、力场和正在使用的其他功能的具体情况。Snippet 78 全面概述了 Lammps 中的 GPU 包，指出它支持 NVIDIA（通过 CUDA）、AMD 和更通用的 OpenCL，使其可以在各种 GPU 硬件（包括某些 CPU 上的集成 GPU）上运行。它还提到，多个 MPI 任务可以共享 Lammps 中的单个 GPU。Snippet 76 展示了 Lammps 在 NVIDIA A10、A40 和 A100 GPU 上的 GPU 扩展基准测试结果，结果表明，与仅使用 CPU 的运行相比，Lennard-Jones 基准测试的性能得到了显著提高，其中 A100 的性能最高。Snippet 79 指出，要通过使用 GPU（通过 KOKKOS 和 GPU 包）在 Lammps 中实现显著加速，需要具有快速设备内存和高效数据传输速率的强大 GPU，这表明通常需要中高端桌面 GPU 才能看到显著的优势。性能较低的 GPU（如笔记本电脑中的 GPU）甚至可能导致速度减慢。  
* 7.4 大型原子模拟：  
  讨论 Lammps 适用于模拟从纳米级到宏观级的各种规模的非常大的系统的能力，以及此类模拟的硬件影响 80。Snippet 80 明确指出，Lammps 能够模拟从仅有几个粒子到数十亿个粒子的系统，使其非常适合各种领域的大规模原子模拟。Snippet 81 提到 Lammps 网站提供了简单 Lennard-Jones 模型非常大的基准运行的基准数据，包括包含十亿个原子的模拟，突显了该软件在适当的 HPC 资源上处理极其庞大系统的能力。Snippet 82 比较了用于运行 Lammps 模拟的 H100 和 A100 节点，指出 H100 节点由于具有更多更新一代的 GPU（8 个 H100 GPU 与 2 个 A100 GPU），因此提供了更高的性能潜力，尤其对于可以有效利用多个 GPU 的大规模模拟。  
* 7.5 并行化：  
  简要提及 Lammps 广泛的并行化能力，支持 MPI、OpenMP、向量化和 GPU 加速 5。Snippet 83 和 80 指出，Lammps 专为并行计算机设计，可以使用 MPI 串行或并行运行，并支持 OpenMP 多线程、向量化和 GPU 加速，使其高度适应各种 HPC 环境。Snippet 5 讨论了 HPC 中常用的并行化策略，包括使用 MPI 在多个核心和节点之间分配工作负载，以及使用 CUDA/OpenCL 利用 GPU 的并行处理能力，Lammps 都支持这两种策略。Snippet 84 提到 Lammps 使用 3-D 空间域分解将原子分布到 MPI 进程中以进行并行执行，这是一种常见且有效的基于粒子的模拟技术。

**8\. 比较分析和综合建议**

* **表：计算化学和材料科学软件的推荐硬件配置**

| 软件 | 计算规模 | 理论方法级别 | 推荐 CPU (型号/核心数) | 推荐内存 (RAM) | 推荐 GPU (型号/数量) |
| :---- | :---- | :---- | :---- | :---- | :---- |
| VASP | 小体系 (\<100) | 低精度 (GGA) | Intel Core i7/Ryzen 7 (8-16 核) | 32-64 GB | 适中 GPU (例如 RTX 3060 Ti/RX 6800 XT) |
|  | 中等体系 (100-500) | 中精度 (metaGGA, Hybrid GGA) | Intel Xeon/AMD EPYC (16-32 核) | 64-256 GB | 高端 GPU (例如 RTX 3080/RTX 4070\) |
|  | 大体系 (500-1000) | 高精度 (double Hybrid GGA, CCSD, RPA) | AMD EPYC (32-64 核) | 256-512 GB | 高端 GPU (例如 RTX 3090/RTX 4080\) |
|  | 超大体系 (\>1000) | 高精度 (double Hybrid GGA, CCSD, RPA) | 多节点集群，AMD EPYC (每节点 64+ 核) | 512 GB+ | 多 GPU (例如 2-4x RTX 3090/RTX 4090 或专业级 GPU) |
| CP2K | 小体系 (\<100) | 低精度 (DFT) | Intel Core i7/Ryzen 7 (8-16 核) | 32-64 GB | 适中 GPU (例如 RTX 3060 Ti/RX 6800 XT) |
|  | 中等体系 (100-500) | 中精度 (DFT) | Intel Xeon/AMD EPYC (16-32 核) | 64-256 GB | 高端 GPU (例如 RTX 3080/RTX 4070\) |
|  | 大体系 (500-1000) | 高精度 (MP2, RPA) | AMD EPYC (32-64 核) 或 Intel Xeon Max (高带宽内存) | 256-512 GB | 高端 GPU (例如 RTX 3090/RTX 4080 或专业级 GPU) |
|  | 超大体系 (\>1000) | 高精度 (MP2, RPA) | 多节点集群，AMD EPYC (每节点 64+ 核) 或 Intel Xeon Max | 512 GB+ | 多 GPU (例如 2-4x RTX 3090/RTX 4090 或专业级 GPU) |
| QE | 小体系 (\<100) | 低精度 (LDA, GGA) | Intel Core i7/Ryzen 7 (8-16 核) | 32-64 GB | 适中 GPU (例如 RTX 3060 Ti/RX 6800 XT) |
|  | 中等体系 (100-500) | 中精度 (Hybrid functionals) | Intel Xeon/AMD EPYC (16-32 核) | 64-256 GB | 高端 GPU (例如 NVIDIA Tesla P100/V100) |
|  | 大体系 (500-1000) | 高精度 (Hybrid functionals) | AMD EPYC (32-64 核) | 256-512 GB | 高端 GPU (例如 NVIDIA Tesla P100/V100 或更高) |
|  | 超大体系 (\>1000) | 高精度 (Hybrid functionals) | 多节点集群，AMD EPYC (每节点 64+ 核) | 512 GB+ | 多 GPU (例如 2-4x NVIDIA Tesla V100/A100) |
| Gaussian | 小体系 (\<100) | 低精度 (DFT, HF) | 高主频 Intel Core i7/Ryzen 7 (8-16 核) | 32-64 GB | 适中 GPU (用于可视化) |
|  | 中等体系 (100-500) | 中精度 (DFT) | 高主频 Intel Xeon/AMD EPYC (16-32 核) | 64-256 GB | 高端 GPU (用于 DFT 计算) |
|  | 大体系 (500-1000) | 高精度 (MP2, CCSD) | 高主频 Intel Xeon/AMD EPYC (32-64 核) | 256-512 GB | (GPU 对 MP2/CCSD 效果有限，NVMe SSD 重要) |
|  | 超大体系 (\>1000) | 高精度 (MP2, CCSD) | 多节点集群，高主频 Intel Xeon (每节点 32+ 核) | 512 GB+ | (GPU 对 MP2/CCSD 效果有限，NVMe SSD 重要) |
| Gromacs | 小体系 (\<100) | 分子动力学 | 高主频 Intel Core i7/Ryzen 7 (8-16 核) | 16-32 GB | 适中 GPU (例如 RTX 3060 Ti/RX 6800 XT) |
|  | 中等体系 (100-500) | 分子动力学 | 高主频 Intel Xeon/AMD EPYC (16-32 核) | 32-64 GB | 高端 GPU (例如 RTX 3080/RTX 4070\) |
|  | 大体系 (500-1000) | 分子动力学 | 高主频 AMD EPYC (32-64 核) | 64-128 GB | 高端 GPU (例如 RTX 3090/RTX 4080\) |
|  | 超大体系 (\>1000) | 分子动力学 | 多节点集群，高主频 AMD EPYC (每节点 64+ 核) | 128 GB+ | 多 GPU (例如 2-4x RTX 3090/RTX 4090 或专业级 GPU) |
| Lammps | 小体系 (\<100) | 分子动力学 | 高主频 Intel Core i7/Ryzen 7 (8-16 核) | 16-32 GB | 适中 GPU (例如 RTX 3060 Ti/RX 6800 XT) |
|  | 中等体系 (100-500) | 分子动力学 | 高主频 Intel Xeon/AMD EPYC (16-32 核) | 32-64 GB | 高端 GPU (例如 RTX 3080/RTX 4070\) |
|  | 大体系 (500-1000) | 分子动力学 | 高主频 AMD EPYC (32-64 核) | 64-128 GB | 高端 GPU (例如 RTX 3090/RTX 4080\) |
|  | 超大体系 (\>1000) | 分子动力学 | 多节点集群，高主频 AMD EPYC/Intel Xeon (每节点 64+ 核) | 128 GB+ | 多 GPU (例如 2-8x RTX 3090/RTX 4090 或专业级 GPU) |

**9\. 第一性原理计算的通用硬件推荐和基准测试**

* 9.1 总体硬件考虑因素：  
  讨论第一性原理计算中硬件的一般趋势和建议，强调 CPU 性能（主频和核心数）、充足的高带宽内存以及日益增长的 GPU 加速利用以增强计算能力之间的平衡 1。Snippet 85 推荐使用单路 Intel Xeon W 或 AMD Threadripper PRO 处理器作为科学计算工作站的首选，这是因为它们支持多个内存通道，这对于第一性原理计算中常见的内存密集型应用程序至关重要，并且还建议考虑企业级平台，以确保在持续高计算负载下的鲁棒性。Snippet 34 建议使用 Intel Core i7 或 Ryzen 7 处理器，搭配至少 32GB 高速 DDR4 RAM 和中等性能的 GPU（如 NVIDIA GeForce RTX 3060 Ti 或 Radeon RX 6800 XT），作为量子化学计算（通常依赖于第一性原理方法）入门级 PC 的坚实基础。Snippet 86 强调了利用 GPGPU（通用图形处理器）来显著提高第一性原理计算软件（如基于线性原子轨道 (LCAO) 基组的 ABACUS）的计算速度的日益增长的趋势，表明 GPU 加速在该领域的重要性日益提高。Snippet 7 强调，成功的计算化学（包括第一性原理计算）依赖于快速 CPU、充足且高带宽的内存，以及日益强大的 GPU 来处理所涉及的大型数据集和复杂计算，还建议为此类工作负载使用稳定且灵活的基于 Linux 的操作系统。Snippets 1 和 2 将基于密度泛函理论 (DFT) 的第一性原理计算引入为现代材料设计和研究的基石，强调了这些方法固有的计算密集性以及高性能计算 (HPC) 资源在将其应用于复杂和实际材料系统中的关键作用。  
* 9.2 相关基准测试报告：  
  讨论基准测试在评估用于第一性原理计算的 HPC 系统性能方面的重要性，注意可用的各种基准测试以及选择那些代表特定应用和感兴趣工作负载的基准测试的必要性 87。Snippet 87 概述了 HPC 基准测试的一般情况，强调每个科学应用都有其独特的性能瓶颈，因此，创建有用的基准测试需要测量足够范围的系统方面，以捕捉被评估的硬件和软件的复杂性。Snippet 88 展示了在 Cray HPC 系统上对三种流行的计算化学软件包（NWChem、Gromacs 和 Lammps）的性能研究，提供了这些代码在不同超级计算架构上的扩展行为和性能特征的见解，这可能与理解第一性原理计算的一般性能趋势相关。Snippet 89 介绍了各种已建立的 HPC 基准测试，包括 LINPACK（侧重于浮点运算）、STREAM（测量内存带宽）和 SPEC（评估应用程序性能），突出了它们不同的侧重点以及考虑多个基准测试以全面了解系统能力的重要性。Snippet 90 描述了 SPEChpc 2021 基准测试套件，该套件专门用于解决评估现代 HPC 系统性能的挑战，这些系统越来越多地包含异构架构，例如具有各种类型的加速器，使其特别适用于评估旨在利用 GPU 或其他加速器的第一性原理计算的系统。Snippet 91（来自 NVIDIA）强调了通过使用 GPU 加速服务器为各种 HPC 应用（包括工程、地球科学和物理学）实现的显著性能提升，展示了 GPU 在显著提高与材料科学和化学相关的科学模拟的吞吐量和降低计算成本方面的潜力。Snippet 92 引用了 OpenBenchmarking.org，这是一个用于查找和比较各种 HPC 应用（包括 Lammps）在各种硬件配置上的基准测试结果的宝贵资源，提供了一个社区驱动的性能评估平台。Snippet 93 指导用户访问 LAMMPS 网站，该网站提供了软件在不同 CPU 架构、GPU 和 Intel Xeon Phi 协处理器上针对一系列标准基准问题的详细性能基准测试，为该分子动力学代码（通常用于材料科学研究）的用户提供了直接的性能数据来源。Snippet 94 指向 Quantum ESPRESSO 基金会的 GitHub 存储库，作为获取与 QE 软件相关的基准测试数据的来源，允许用户访问在各种硬件平台上针对不同类型的第一性原理计算获得的性能结果。

**10\. 结论**

总结了针对每种涵盖的软件包（VASP、CP2K、Quantum ESPRESSO、Gaussian、Gromacs、Lammps）的关键硬件建议，强调了平衡 CPU 核心数和时钟速度与足够的内存容量和带宽的重要性，以及在特定方法和问题规模中战略性地使用 GPU 加速。重申了研究中观察到的一般趋势，包括越来越依赖 GPU 加速来实现计算化学和材料科学中的高性能、内存带宽对于许多类型的第一性原理计算的关键作用，以及仔细考虑每种软件包相对于 CPU 核心数和多节点并行性的扩展特性的必要性。强调在做出硬件选择时，用户需要考虑其特定的计算需求、预算限制以及其模拟系统的具体特性，因为这些因素会显著影响最佳硬件配置。最后，强烈建议研究人员和计算资源管理者在可行的情况下，使用其预期硬件对代表性工作负载进行自己的基准测试，因为经验验证通常是确保所选硬件能够为其特定科学应用提供所需性能的最可靠方法。

#### **引用的著作**

1. (PDF) First-principles materials-simulation technology \- ResearchGate, 访问时间为 四月 29, 2025， [https://www.researchgate.net/publication/281891608\_First-principles\_materials-simulation\_technology](https://www.researchgate.net/publication/281891608_First-principles_materials-simulation_technology)  
2. Materials Databases Infrastructure Constructed by First Principles Calculations: A Review \- OSTI, 访问时间为 四月 29, 2025， [https://www.osti.gov/servlets/purl/1223664](https://www.osti.gov/servlets/purl/1223664)  
3. First-Principles Calculations on Electronic, Optical, and Phonon Properties of γ-Bi2MoO6 | ACS Omega \- ACS Publications, 访问时间为 四月 29, 2025， [https://pubs.acs.org/doi/10.1021/acsomega.4c03171](https://pubs.acs.org/doi/10.1021/acsomega.4c03171)  
4. JayLau123/HPC-for-materials: High performance computing for materials simulation. The 3rd paradigm. \- GitHub, 访问时间为 四月 29, 2025， [https://github.com/JayLau123/HPC-for-materials](https://github.com/JayLau123/HPC-for-materials)  
5. Connecting performance to hardware – Running LAMMPS on HPC systems, 访问时间为 四月 29, 2025， [http://www.hpc-carpentry.org/tuning\_lammps/02-hardware-performance/index.html](http://www.hpc-carpentry.org/tuning_lammps/02-hardware-performance/index.html)  
6. Running LAMMPS on HPC systems, 访问时间为 四月 29, 2025， [https://www.hpc-carpentry.org/tuning\_lammps/aio/index.html](https://www.hpc-carpentry.org/tuning_lammps/aio/index.html)  
7. 1.3 Hardware and software requirements for computational chemistry \- Fiveable, 访问时间为 四月 29, 2025， [https://library.fiveable.me/computational-chemistry/unit-1/hardware-software-requirements-computational-chemistry/study-guide/llJPWKT2CNn44jzt](https://library.fiveable.me/computational-chemistry/unit-1/hardware-software-requirements-computational-chemistry/study-guide/llJPWKT2CNn44jzt)  
8. Do you necessarily need a supercomputer to run molecular dynamics simulation?, 访问时间为 四月 29, 2025， [https://www.researchgate.net/post/Do-you-necessarily-need-a-supercomputer-to-run-molecular-dynamics-simulation](https://www.researchgate.net/post/Do-you-necessarily-need-a-supercomputer-to-run-molecular-dynamics-simulation)  
9. Hardware Configuration Advice for DFT and GW Calculations with ..., 访问时间为 四月 29, 2025， [https://vasp.at/forum/viewtopic.php?p=26132](https://vasp.at/forum/viewtopic.php?p=26132)  
10. Hardware recommendations for VASP \- Peter Larsson \- NSC, 访问时间为 四月 29, 2025， [https://www.nsc.liu.se/\~pla/blog/2013/09/04/hardware-for-vasp/](https://www.nsc.liu.se/~pla/blog/2013/09/04/hardware-for-vasp/)  
11. computing performance of vasp on modern high end systems \- My Community, 访问时间为 四月 29, 2025， [https://ww.vasp.at/viewtopic.php?t=10247](https://ww.vasp.at/viewtopic.php?t=10247)  
12. memory requirement .... \- My Community \- VASP, 访问时间为 四月 29, 2025， [https://www.vasp.at/viewtopic.php?t=10667](https://www.vasp.at/viewtopic.php?t=10667)  
13. quantum espresso \- How many GB of RAM would be needed for VASP to evaluate optical properties of a material? \- Matter Modeling Stack Exchange, 访问时间为 四月 29, 2025， [https://mattermodeling.stackexchange.com/questions/3786/how-many-gb-of-ram-would-be-needed-for-vasp-to-evaluate-optical-properties-of-a](https://mattermodeling.stackexchange.com/questions/3786/how-many-gb-of-ram-would-be-needed-for-vasp-to-evaluate-optical-properties-of-a)  
14. Running VASP on Nvidia GPUs \- Peter Larsson \- NSC, 访问时间为 四月 29, 2025， [https://www.nsc.liu.se/\~pla/blog/2015/11/16/vaspgpu/](https://www.nsc.liu.se/~pla/blog/2015/11/16/vaspgpu/)  
15. cug.org, 访问时间为 四月 29, 2025， [https://cug.org/proceedings/cug2023\_proceedings/includes/files/pap130s2-file1.pdf](https://cug.org/proceedings/cug2023_proceedings/includes/files/pap130s2-file1.pdf)  
16. CPU and GPU performance in VASP compilation \- Materials Science Community Discourse, 访问时间为 四月 29, 2025， [https://matsci.org/t/cpu-and-gpu-performance-in-vasp-compilation/57526](https://matsci.org/t/cpu-and-gpu-performance-in-vasp-compilation/57526)  
17. VASP on a GPU \- Carnegie Mellon University, 访问时间为 四月 29, 2025， [https://euler.phys.cmu.edu/widom/pubs/PDF/ComPhysCom183\_2012\_1422.pdf](https://euler.phys.cmu.edu/widom/pubs/PDF/ComPhysCom183_2012_1422.pdf)  
18. VASP 6 performance on CTE-POWER | BSC Support Knowledge Center, 访问时间为 四月 29, 2025， [https://www.bsc.es/supportkc/tech-reports/vasp6-report](https://www.bsc.es/supportkc/tech-reports/vasp6-report)  
19. Impact of NVLink and Multi-GPU Scaling on Efficiency for Various Job Sizes in VASP GPU Edition. \- My Community, 访问时间为 四月 29, 2025， [https://ww.vasp.at/viewtopic.php?t=19531](https://ww.vasp.at/viewtopic.php?t=19531)  
20. How can I maximize both CPU and GPU usage when running OpenACC VASP?, 访问时间为 四月 29, 2025， [https://mattermodeling.stackexchange.com/questions/11772/how-can-i-maximize-both-cpu-and-gpu-usage-when-running-openacc-vasp](https://mattermodeling.stackexchange.com/questions/11772/how-can-i-maximize-both-cpu-and-gpu-usage-when-running-openacc-vasp)  
21. Scaling VASP with NVIDIA Magnum IO | NVIDIA Technical Blog, 访问时间为 四月 29, 2025， [https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/](https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/)  
22. Optimising CP2K for the Intel Xeon Phi \- Partnership for Advanced Computing in Europe \- PRACE, 访问时间为 四月 29, 2025， [https://prace-ri.eu/wp-content/uploads/wp140.pdf](https://prace-ri.eu/wp-content/uploads/wp140.pdf)  
23. Performance Analysis of CP2K Code for Ab Initio Molecular Dynamics \- arXiv, 访问时间为 四月 29, 2025， [https://arxiv.org/pdf/2109.04536](https://arxiv.org/pdf/2109.04536)  
24. Performance Analysis of CP2K Code for Ab Initio Molecular Dynamics \- ResearchGate, 访问时间为 四月 29, 2025， [https://www.researchgate.net/publication/354542609\_Performance\_Analysis\_of\_CP2K\_Code\_for\_Ab\_Initio\_Molecular\_Dynamics](https://www.researchgate.net/publication/354542609_Performance_Analysis_of_CP2K_Code_for_Ab_Initio_Molecular_Dynamics)  
25. CP2K Dev Meeting February 2023, 访问时间为 四月 29, 2025， [https://www.cp2k.org/\_media/dev:cp2k\_dev\_meeting\_06\_02\_23.pdf](https://www.cp2k.org/_media/dev:cp2k_dev_meeting_06_02_23.pdf)  
26. CP2K on 5th Gen AMD EPYC™ Processors, 访问时间为 四月 29, 2025， [https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/performance-briefs/amd-epyc-9005-pb-cp2k.pdf](https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/performance-briefs/amd-epyc-9005-pb-cp2k.pdf)  
27. Nvidia Pushes Hopper HBM Memory, And That Lifts GPU Performance \- The Next Platform, 访问时间为 四月 29, 2025， [https://www.nextplatform.com/2023/11/13/nvidia-pushes-hopper-hbm-memory-and-that-lifts-gpu-performance/](https://www.nextplatform.com/2023/11/13/nvidia-pushes-hopper-hbm-memory-and-that-lifts-gpu-performance/)  
28. CP2K performance on GPUs \- Google Groups, 访问时间为 四月 29, 2025， [https://groups.google.com/g/cp2k/c/P-V-3wknsKs/m/kivPi1jTAwAJ](https://groups.google.com/g/cp2k/c/P-V-3wknsKs/m/kivPi1jTAwAJ)  
29. cp2k \- NGC Catalog \- NVIDIA, 访问时间为 四月 29, 2025， [https://catalog.ngc.nvidia.com/orgs/hpc/containers/cp2k](https://catalog.ngc.nvidia.com/orgs/hpc/containers/cp2k)  
30. Running Cp2k in parallel using thread in a PC \- Google Groups, 访问时间为 四月 29, 2025， [https://groups.google.com/g/cp2k/c/1MSVLN71m14](https://groups.google.com/g/cp2k/c/1MSVLN71m14)  
31. CP2K PERFORMANCE ON ARCHER, 访问时间为 四月 29, 2025， [https://www.cp2k.org/\_media/events:2014\_user\_meeting:cp2k-uk-2014-reid.pdf](https://www.cp2k.org/_media/events:2014_user_meeting:cp2k-uk-2014-reid.pdf)  
32. cp2k/benchmarks/QS\_mp2\_rpa/64-H2O/README.md at master \- GitHub, 访问时间为 四月 29, 2025， [https://github.com/cp2k/cp2k/blob/master/benchmarks/QS\_mp2\_rpa/64-H2O/README.md](https://github.com/cp2k/cp2k/blob/master/benchmarks/QS_mp2_rpa/64-H2O/README.md)  
33. exercises:2015\_pitt:mp2 \[CP2K Open Source Molecular Dynamics \], 访问时间为 四月 29, 2025， [https://www.cp2k.org/exercises:2015\_pitt:mp2](https://www.cp2k.org/exercises:2015_pitt:mp2)  
34. Can anyone guide me with the selection of good begineers pc or laptop specifications for computational calculations like quantum expresso, vasp? | ResearchGate, 访问时间为 四月 29, 2025， [https://www.researchgate.net/post/Can\_anyone\_guide\_me\_with\_the\_selection\_of\_good\_begineers\_pc\_or\_laptop\_specifications\_for\_computational\_calculations\_like\_quantum\_expresso\_vasp](https://www.researchgate.net/post/Can_anyone_guide_me_with_the_selection_of_good_begineers_pc_or_laptop_specifications_for_computational_calculations_like_quantum_expresso_vasp)  
35. Desktop PC\\Workstation \<1600$ (for Quantum Espresso calculations) : r/buildapc \- Reddit, 访问时间为 四月 29, 2025， [https://www.reddit.com/r/buildapc/comments/155uyhg/desktop\_pcworkstation\_1600\_for\_quantum\_espresso/](https://www.reddit.com/r/buildapc/comments/155uyhg/desktop_pcworkstation_1600_for_quantum_espresso/)  
36. Quantum ESPRESSO on 5th Gen AMD EPYC™ Processors, 访问时间为 四月 29, 2025， [https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/performance-briefs/amd-epyc-9005-pb-quantum-espresso.pdf](https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/performance-briefs/amd-epyc-9005-pb-quantum-espresso.pdf)  
37. Quantum Espresso in the United European Applications Benchmark Suite (UEABS), 访问时间为 四月 29, 2025， [https://repository.prace-ri.eu/git/UEABS/ueabs/-/blob/02de12788523efea01c1ef8307a43d141416f2fc/quantum\_espresso/README.md](https://repository.prace-ri.eu/git/UEABS/ueabs/-/blob/02de12788523efea01c1ef8307a43d141416f2fc/quantum_espresso/README.md)  
38. System Requirements for QuantumATK Q-2019.12, 访问时间为 四月 29, 2025， [https://docs.quantumatk.com/faq/faq\_technical\_hardware\_for\_q2019\_12.html](https://docs.quantumatk.com/faq/faq_technical_hardware_for_q2019_12.html)  
39. quantum ESPRESSO & GPU survival guide \- EPW, 访问时间为 四月 29, 2025， [https://docs.epw-code.org/\_downloads/e9a4fa9f61302bb55b5f804bcc805880/Sat.1.Giannozzi.pdf](https://docs.epw-code.org/_downloads/e9a4fa9f61302bb55b5f804bcc805880/Sat.1.Giannozzi.pdf)  
40. Quantum ESPRESSO: One Further Step toward the Exascale | Journal of Chemical Theory and Computation, 访问时间为 四月 29, 2025， [https://pubs.acs.org/doi/10.1021/acs.jctc.3c00249](https://pubs.acs.org/doi/10.1021/acs.jctc.3c00249)  
41. Running Quantum ESPRESSO on a GPU in Windows \- Matter Modeling Stack Exchange, 访问时间为 四月 29, 2025， [https://mattermodeling.stackexchange.com/questions/6126/running-quantum-espresso-on-a-gpu-in-windows](https://mattermodeling.stackexchange.com/questions/6126/running-quantum-espresso-on-a-gpu-in-windows)  
42. arXiv:2104.10502v2 \[physics.comp-ph\] 22 Apr 2021, 访问时间为 四月 29, 2025， [https://arxiv.org/pdf/2104.10502](https://arxiv.org/pdf/2104.10502)  
43. A performance study of Quantum ESPRESSO's PWscf code on multi-core and GPU systems, 访问时间为 四月 29, 2025， [https://www.dcs.warwick.ac.uk/pmbs/pmbs17/PMBS/papers/paper3.pdf](https://www.dcs.warwick.ac.uk/pmbs/pmbs17/PMBS/papers/paper3.pdf)  
44. Hubbard, Hybrid and Nonlocal Functionals in quantum ESPRESSO, 访问时间为 四月 29, 2025， [http://qe2019.ijs.si/talks/Giannozzi\_Day2-2.pdf](http://qe2019.ijs.si/talks/Giannozzi_Day2-2.pdf)  
45. Band structure calculations in QE using hybrid functionals \- Christoph Wolf, 访问时间为 四月 29, 2025， [https://christoph-wolf.at/2018/10/10/band-structure-calculations-in-qe-using-hybrid-functionals/](https://christoph-wolf.at/2018/10/10/band-structure-calculations-in-qe-using-hybrid-functionals/)  
46. Brief recap on KS-DFT. KS equations, common approximations to the xc energy \- Materials Cloud, 访问时间为 四月 29, 2025， [https://www.materialscloud.org/learn/data/learn/files/bU1O9b2BiFFk/QE2022D1\_Nicola\_Colonna\_and\_Iurii\_Timrov\_handson.pdf](https://www.materialscloud.org/learn/data/learn/files/bU1O9b2BiFFk/QE2022D1_Nicola_Colonna_and_Iurii_Timrov_handson.pdf)  
47. Query on using the exchange-correlation functional GGA or LDA approximation in Quantum ESPRESSO for electronic and magnetic property calculations?, 访问时间为 四月 29, 2025， [https://mattermodeling.stackexchange.com/questions/10513/query-on-using-the-exchange-correlation-functional-gga-or-lda-approximation-in-q](https://mattermodeling.stackexchange.com/questions/10513/query-on-using-the-exchange-correlation-functional-gga-or-lda-approximation-in-q)  
48. What will be the better approach to calculation in Quantum Espresso? \- ResearchGate, 访问时间为 四月 29, 2025， [https://www.researchgate.net/post/What\_will\_be\_the\_better\_approach\_to\_calculation\_in\_Quantum\_Espresso](https://www.researchgate.net/post/What_will_be_the_better_approach_to_calculation_in_Quantum_Espresso)  
49. Deciding the number of CPUs for a DFT calculation for band structure in Quantum ESPRESSO? \- Matter Modeling Stack Exchange, 访问时间为 四月 29, 2025， [https://mattermodeling.stackexchange.com/questions/3816/deciding-the-number-of-cpus-for-a-dft-calculation-for-band-structure-in-quantum](https://mattermodeling.stackexchange.com/questions/3816/deciding-the-number-of-cpus-for-a-dft-calculation-for-band-structure-in-quantum)  
50. I am planning to procure a computer system to run gaussian program of big biomolecules..can anyone suggest me the necessary specification for this? | ResearchGate, 访问时间为 四月 29, 2025， [https://www.researchgate.net/post/I\_am\_planning\_to\_procure\_a\_computer\_system\_to\_run\_gaussian\_program\_of\_big\_biomoleculescan\_anyone\_suggest\_me\_the\_necessary\_specification\_for\_this](https://www.researchgate.net/post/I_am_planning_to_procure_a_computer_system_to_run_gaussian_program_of_big_biomoleculescan_anyone_suggest_me_the_necessary_specification_for_this)  
51. HowTo:gaussian:release \- CAC Wiki, 访问时间为 四月 29, 2025， [https://info.cac.queensu.ca/wiki/index.php/HowTo:gaussian:release](https://info.cac.queensu.ca/wiki/index.php/HowTo:gaussian:release)  
52. G16W System Requirements \- Gaussian.com, 访问时间为 四月 29, 2025， [https://gaussian.com/g16wplat/](https://gaussian.com/g16wplat/)  
53. Running Gaussian, 访问时间为 四月 29, 2025， [https://gaussian.com/running/](https://gaussian.com/running/)  
54. (Novice) How to maximize Gaussian 16 memory allocation? \- ResearchGate, 访问时间为 四月 29, 2025， [https://www.researchgate.net/post/Novice\_How\_to\_maximize\_Gaussian\_16\_memory\_allocation](https://www.researchgate.net/post/Novice_How_to_maximize_Gaussian_16_memory_allocation)  
55. Gaussian \- Docs CSC, 访问时间为 四月 29, 2025， [https://docs.csc.fi/apps/gaussian/](https://docs.csc.fi/apps/gaussian/)  
56. Gaussian 16 Features at a Glance, 访问时间为 四月 29, 2025， [https://gaussian.com/g16glance/](https://gaussian.com/g16glance/)  
57. Using GPUs | Gaussian.com, 访问时间为 四月 29, 2025， [https://gaussian.com/gpu/](https://gaussian.com/gpu/)  
58. Using Gaussian 09 with Linda, 访问时间为 四月 29, 2025， [https://www.csus.edu/indiv/g/ghermanb/research\_foo/g09ur/m\_linda.htm](https://www.csus.edu/indiv/g/ghermanb/research_foo/g09ur/m_linda.htm)  
59. How many processors and how much memory should I request for a Gaussian calculation?, 访问时间为 四月 29, 2025， [https://ask.cyberinfrastructure.org/t/how-many-processors-and-how-much-memory-should-i-request-for-a-gaussian-calculation/232](https://ask.cyberinfrastructure.org/t/how-many-processors-and-how-much-memory-should-i-request-for-a-gaussian-calculation/232)  
60. Gaussian \- Research Computing University of Colorado Boulder documentation, 访问时间为 四月 29, 2025， [https://curc.readthedocs.io/en/latest/software/gaussian.html](https://curc.readthedocs.io/en/latest/software/gaussian.html)  
61. Best CPU, GPU, RAM for Molecular Dynamics | SabrePC Blog, 访问时间为 四月 29, 2025， [https://www.sabrepc.com/blog/life-sciences/best-cpu-gpu-and-ram-for-md-workstation-server](https://www.sabrepc.com/blog/life-sciences/best-cpu-gpu-and-ram-for-md-workstation-server)  
62. GROMACS GPU Benchmark and Hardware Recommendations | Exxact Blog, 访问时间为 四月 29, 2025， [https://www.exxactcorp.com/blog/benchmarks/gromacs-gpu-benchmark-and-hardware-recommendations](https://www.exxactcorp.com/blog/benchmarks/gromacs-gpu-benchmark-and-hardware-recommendations)  
63. RTX 3090 vs RTX 3080//CPU-GPU ratio//best performance \- GROMACS forums, 访问时间为 四月 29, 2025， [https://gromacs.bioexcel.eu/t/rtx-3090-vs-rtx-3080-cpu-gpu-ratio-best-performance/867](https://gromacs.bioexcel.eu/t/rtx-3090-vs-rtx-3080-cpu-gpu-ratio-best-performance/867)  
64. Efficient number of cores for a GROMACS simulation : r/comp\_chem \- Reddit, 访问时间为 四月 29, 2025， [https://www.reddit.com/r/comp\_chem/comments/1bs27la/efficient\_number\_of\_cores\_for\_a\_gromacs\_simulation/](https://www.reddit.com/r/comp_chem/comments/1bs27la/efficient_number_of_cores_for_a_gromacs_simulation/)  
65. Recommended RAM size for GROMACS workstation \- User discussions, 访问时间为 四月 29, 2025， [https://gromacs.bioexcel.eu/t/recommended-ram-size-for-gromacs-workstation/9613](https://gromacs.bioexcel.eu/t/recommended-ram-size-for-gromacs-workstation/9613)  
66. What should be the required RAM for the size of the atoms (thousands to crores or millions) for performing MDS in Gromacs? | ResearchGate, 访问时间为 四月 29, 2025， [https://www.researchgate.net/post/What\_should\_be\_the\_required\_RAM\_for\_the\_size\_of\_the\_atoms\_thousands\_to\_crores\_or\_millions\_for\_performing\_MDS\_in\_Gromacs](https://www.researchgate.net/post/What_should_be_the_required_RAM_for_the_size_of_the_atoms_thousands_to_crores_or_millions_for_performing_MDS_in_Gromacs)  
67. Best bang for your buck: GPU nodes for GROMACS biomolecular simulations \- PMC, 访问时间为 四月 29, 2025， [https://pmc.ncbi.nlm.nih.gov/articles/PMC5042102/](https://pmc.ncbi.nlm.nih.gov/articles/PMC5042102/)  
68. GROMACS performance on different GPU types \- NHR@FAU, 访问时间为 四月 29, 2025， [https://hpc.fau.de/2022/02/10/gromacs-performance-on-different-gpu-types/](https://hpc.fau.de/2022/02/10/gromacs-performance-on-different-gpu-types/)  
69. Best graphics card for running gromacs \- NVIDIA Developer Forums, 访问时间为 四月 29, 2025， [https://forums.developer.nvidia.com/t/best-graphics-card-for-running-gromacs/64208](https://forums.developer.nvidia.com/t/best-graphics-card-for-running-gromacs/64208)  
70. Step-by-Step Guide to Installing Gromacs with GPU Acceleration, 访问时间为 四月 29, 2025， [https://parssilico.com/blogs/98-installing-gromacs-with-gpu-acceleration](https://parssilico.com/blogs/98-installing-gromacs-with-gpu-acceleration)  
71. A free GROMACS benchmark set \- MPI, 访问时间为 四月 29, 2025， [https://www.mpinat.mpg.de/grubmueller/bench](https://www.mpinat.mpg.de/grubmueller/bench)  
72. GROMACS scaling limit for a large system \- User discussions, 访问时间为 四月 29, 2025， [https://gromacs.bioexcel.eu/t/gromacs-scaling-limit-for-a-large-system/1454](https://gromacs.bioexcel.eu/t/gromacs-scaling-limit-for-a-large-system/1454)  
73. Getting good performance from mdrun — GROMACS 5.1.1 documentation, 访问时间为 四月 29, 2025， [https://manual.gromacs.org/5.1.1/user-guide/mdrun-performance.html](https://manual.gromacs.org/5.1.1/user-guide/mdrun-performance.html)  
74. Getting good performance from mdrun — GROMACS 5.1 documentation, 访问时间为 四月 29, 2025， [https://manual.gromacs.org/5.1/user-guide/mdrun-performance.html](https://manual.gromacs.org/5.1/user-guide/mdrun-performance.html)  
75. Optimum number of CPUs to use for an efficient LAMMPS simulation, 访问时间为 四月 29, 2025， [https://mattermodeling.stackexchange.com/questions/12775/optimum-number-of-cpus-to-use-for-an-efficient-lammps-simulation](https://mattermodeling.stackexchange.com/questions/12775/optimum-number-of-cpus-to-use-for-an-efficient-lammps-simulation)  
76. Running LAMMPS on CCAST Clusters \- NDSU IT Knowledge Base, 访问时间为 四月 29, 2025， [https://kb.ndsu.edu/it/page.php?id=132064](https://kb.ndsu.edu/it/page.php?id=132064)  
77. \[lammps-users\] better Hardware \- Materials Science Community Discourse, 访问时间为 四月 29, 2025， [https://matsci.org/t/lammps-users-better-hardware/35645](https://matsci.org/t/lammps-users-better-hardware/35645)  
78. 7.4.1. GPU package \- LAMMPS documentation, 访问时间为 四月 29, 2025， [https://docs.lammps.org/Speed\_gpu.html](https://docs.lammps.org/Speed_gpu.html)  
79. 7.5. Comparison of various accelerator packages \- LAMMPS documentation, 访问时间为 四月 29, 2025， [https://docs.lammps.org/Speed\_compare.html](https://docs.lammps.org/Speed_compare.html)  
80. 1.1. Overview of LAMMPS, 访问时间为 四月 29, 2025， [https://docs.lammps.org/Intro\_overview.html](https://docs.lammps.org/Intro_overview.html)  
81. LAMMPS Benchmarks, 访问时间为 四月 29, 2025， [https://www.lammps.org/bench.html](https://www.lammps.org/bench.html)  
82. Choosing Hardware for LAMMPS Simulation \- Materials Science Community Discourse, 访问时间为 四月 29, 2025， [https://matsci.org/t/choosing-hardware-for-lammps-simulation/55287](https://matsci.org/t/choosing-hardware-for-lammps-simulation/55287)  
83. LAMMPS Molecular Dynamics Simulator, 访问时间为 四月 29, 2025， [https://www.lammps.org/](https://www.lammps.org/)  
84. OLCF-6 LAMMPS Benchmark, 访问时间为 四月 29, 2025， [https://www.olcf.ornl.gov/wp-content/uploads/OLCF-6\_LAMMPS\_description-1.pdf](https://www.olcf.ornl.gov/wp-content/uploads/OLCF-6_LAMMPS_description-1.pdf)  
85. Hardware Recommendations for Scientific Computing \- Puget Systems, 访问时间为 四月 29, 2025， [https://www.pugetsystems.com/solutions/ai-and-hpc-workstations/scientific-computing/hardware-recommendations/](https://www.pugetsystems.com/solutions/ai-and-hpc-workstations/scientific-computing/hardware-recommendations/)  
86. GPU acceleration of LCAO basis set first-principle calculations \- arXiv, 访问时间为 四月 29, 2025， [https://arxiv.org/html/2409.09399v1](https://arxiv.org/html/2409.09399v1)  
87. The ESIF-HPC-2 Benchmark Suite: Preprint \- NREL, 访问时间为 四月 29, 2025， [https://www.nrel.gov/docs/fy20osti/76098.pdf](https://www.nrel.gov/docs/fy20osti/76098.pdf)  
88. Performance Study of Popular Computational Chemistry Software Packages on Cray HPC Systems, 访问时间为 四月 29, 2025， [https://cug.org/proceedings/cug2018\_proceedings/includes/files/pap134s2-file1.pdf](https://cug.org/proceedings/cug2018_proceedings/includes/files/pap134s2-file1.pdf)  
89. High Performance Computing \- Benchmarks, 访问时间为 四月 29, 2025， [https://www-users.york.ac.uk/\~mijp1/teaching/4th\_year\_HPC/lecture\_notes/Benchmarks.pdf](https://www-users.york.ac.uk/~mijp1/teaching/4th_year_HPC/lecture_notes/Benchmarks.pdf)  
90. SPEChpc 2021 \- Standard Performance Evaluation Corporation, 访问时间为 四月 29, 2025， [https://www.spec.org/hpc2021/](https://www.spec.org/hpc2021/)  
91. NVIDIA HPC Application Performance, 访问时间为 四月 29, 2025， [https://developer.nvidia.com/hpc-application-performance](https://developer.nvidia.com/hpc-application-performance)  
92. LAMMPS Molecular Dynamics Simulator Benchmark \- OpenBenchmarking.org, 访问时间为 四月 29, 2025， [https://openbenchmarking.org/test/pts/lammps\&eval=9a6ae65c1fc80899db3293a653d645d2f81fa584](https://openbenchmarking.org/test/pts/lammps&eval=9a6ae65c1fc80899db3293a653d645d2f81fa584)  
93. 7.1. Benchmarks \- LAMMPS documentation, 访问时间为 四月 29, 2025， [https://docs.lammps.org/Speed\_bench.html](https://docs.lammps.org/Speed_bench.html)  
94. benchmarks \- Quantum Espresso, 访问时间为 四月 29, 2025， [https://www.quantum-espresso.org/benchmarks/](https://www.quantum-espresso.org/benchmarks/)  
95. Hardware Configuration Advice for DFT and GW Calculations with VASP. \- My Community, 访问时间为 四月 29, 2025， [https://p.vasp.at/forum/viewtopic.php?t=19480](https://p.vasp.at/forum/viewtopic.php?t=19480)